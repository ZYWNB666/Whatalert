"""ä¼˜åŒ–çš„åŸºäº Redis çš„åˆ†å¸ƒå¼å‘Šè­¦åˆ†ç»„å™¨ - ä½¿ç”¨ Pipeline å’Œå¼‚æ­¥å¤„ç†"""
import json
import time
import asyncio
from typing import List, Dict, Optional, Set
from loguru import logger
import redis.asyncio as redis
from app.models.alert import AlertEvent, AlertRule


class OptimizedAlertGrouper:
    """ä¼˜åŒ–çš„ Redis åˆ†å¸ƒå¼å‘Šè­¦åˆ†ç»„å™¨
    
    æ€§èƒ½ä¼˜åŒ–ç‰¹æ€§ï¼š
    1. Redis Pipeline æ‰¹é‡æ“ä½œ - å‡å°‘ç½‘ç»œå¾€è¿”
    2. å¼‚æ­¥æ‰¹å¤„ç†é˜Ÿåˆ— - éé˜»å¡å¤„ç†
    3. å¹¶å‘æ§åˆ¶ - ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘
    4. æ‰¹é‡è¯»å†™ - å‡å°‘ Redis æ“ä½œæ¬¡æ•°
    5. å†…å­˜ç¼“å­˜ - å‡å°‘é‡å¤è¯»å–
    """
    
    def __init__(self, redis_client: redis.Redis, max_concurrent: int = 100):
        self.redis = redis_client
        self.group_wait = 10  # åˆ†ç»„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        self.group_interval = 30  # åˆ†ç»„é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        self.repeat_interval = 3600  # é‡å¤å‘é€é—´éš”ï¼ˆç§’ï¼‰
        
        # Redis é”®å‰ç¼€
        self.firing_prefix = "alert:group:firing"
        self.recovery_prefix = "alert:group:recovery"
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.batch_size = 50  # æ‰¹å¤„ç†å¤§å°
        self.batch_queue: List[tuple] = []  # æ‰¹å¤„ç†é˜Ÿåˆ—
        self.batch_lock = asyncio.Lock()
        
        # å†…å­˜ç¼“å­˜ï¼ˆå‡å°‘ Redis è¯»å–ï¼‰
        self.group_cache: Dict[str, dict] = {}
        self.cache_ttl = 5  # ç¼“å­˜ TTLï¼ˆç§’ï¼‰
        self.cache_timestamps: Dict[str, float] = {}
        
        logger.info(f"âœ¨ ä¼˜åŒ–å‘Šè­¦åˆ†ç»„å™¨åˆå§‹åŒ–: max_concurrent={max_concurrent}, batch_size={self.batch_size}")
    
    def _get_group_key(self, group_key: str, is_recovery: bool = False) -> str:
        """ç”Ÿæˆåˆ†ç»„ Redis é”®"""
        prefix = self.recovery_prefix if is_recovery else self.firing_prefix
        return f"{prefix}:{group_key}"
    
    def _generate_group_key(self, alert: AlertEvent, rule: AlertRule) -> tuple:
        """
        ç”Ÿæˆåˆ†ç»„é”®
        
        è¿”å›: (group_key, group_labels)
        """
        group_by = rule.route_config.get('group_by', [])
        
        group_parts = [f"rule:{rule.name}"]
        group_labels = {"alertname": rule.name}
        
        for label_key in group_by:
            label_value = alert.labels.get(label_key, "")
            if label_value:
                group_parts.append(f"{label_key}:{label_value}")
                group_labels[label_key] = label_value
        
        group_key = "|".join(group_parts)
        return group_key, group_labels
    
    def _is_cache_valid(self, redis_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if redis_key not in self.cache_timestamps:
            return False
        return (time.time() - self.cache_timestamps[redis_key]) < self.cache_ttl
    
    async def _get_group_from_cache_or_redis(self, redis_key: str) -> Optional[dict]:
        """ä»ç¼“å­˜æˆ– Redis è·å–åˆ†ç»„æ•°æ®"""
        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if self._is_cache_valid(redis_key):
            return self.group_cache.get(redis_key)
        
        # ä» Redis è¯»å–
        group_data = await self.redis.get(redis_key)
        if group_data:
            group = json.loads(group_data)
            # æ›´æ–°ç¼“å­˜
            self.group_cache[redis_key] = group
            self.cache_timestamps[redis_key] = time.time()
            return group
        
        return None
    
    def _invalidate_cache(self, redis_key: str):
        """ä½¿ç¼“å­˜å¤±æ•ˆ"""
        self.group_cache.pop(redis_key, None)
        self.cache_timestamps.pop(redis_key, None)
    
    async def add_alert(self, alert: AlertEvent, rule: AlertRule) -> str:
        """
        æ·»åŠ å‘Šè­¦åˆ°åˆ†ç»„ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        è¿”å›: group_key
        """
        async with self.semaphore:  # å¹¶å‘æ§åˆ¶
            group_key, group_labels = self._generate_group_key(alert, rule)
            redis_key = self._get_group_key(group_key, is_recovery=False)
            
            # ä»ç¼“å­˜æˆ– Redis è·å–ç°æœ‰åˆ†ç»„æ•°æ®
            group = await self._get_group_from_cache_or_redis(redis_key)
            current_time = time.time()
            
            if not group:
                # åˆ›å»ºæ–°åˆ†ç»„
                group = {
                    "group_key": group_key,
                    "group_labels": group_labels,
                    "rule_id": rule.id,
                    "rule_name": rule.name,
                    "alerts": [],
                    "created_at": current_time,
                    "last_updated_at": current_time,
                    "sent": False
                }
                logger.debug(f"åˆ›å»ºæ–°çš„å‘Šè­¦åˆ†ç»„: {group_key}")
            
            # æ·»åŠ å‘Šè­¦ï¼ˆé¿å…é‡å¤ï¼‰
            alert_fingerprints = {a["fingerprint"] for a in group["alerts"]}
            if alert.fingerprint not in alert_fingerprints:
                group["alerts"].append({
                    "fingerprint": alert.fingerprint,
                    "rule_name": alert.rule_name,
                    "severity": alert.severity,
                    "value": alert.value,
                    "labels": alert.labels,
                    "annotations": alert.annotations,
                    "started_at": alert.started_at,
                    "expr": alert.expr,
                    "tenant_id": alert.tenant_id
                })
                group["last_updated_at"] = current_time
                
                # ä½¿ç”¨ Pipeline æ‰¹é‡å†™å…¥
                async with self.redis.pipeline(transaction=True) as pipe:
                    pipe.setex(redis_key, 7200, json.dumps(group))
                    await pipe.execute()
                
                # æ›´æ–°ç¼“å­˜
                self.group_cache[redis_key] = group
                self.cache_timestamps[redis_key] = current_time
                
                logger.debug(f"å‘Šè­¦æ·»åŠ åˆ°åˆ†ç»„: {group_key}, å½“å‰å‘Šè­¦æ•°: {len(group['alerts'])}")
            
            return group_key
    
    async def add_alerts_batch(self, alerts_with_rules: List[tuple]) -> List[str]:
        """
        æ‰¹é‡æ·»åŠ å‘Šè­¦åˆ°åˆ†ç»„ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰
        
        å‚æ•°:
            alerts_with_rules: List[(alert, rule)]
        
        è¿”å›: List[group_key]
        """
        if not alerts_with_rules:
            return []
        
        logger.info(f"ğŸš€ æ‰¹é‡æ·»åŠ  {len(alerts_with_rules)} ä¸ªå‘Šè­¦åˆ°åˆ†ç»„å™¨")
        start_time = time.time()
        
        # æŒ‰åˆ†ç»„é”®åˆ†ç»„å‘Šè­¦
        groups_map: Dict[str, dict] = {}
        redis_keys_to_fetch: Set[str] = set()
        
        for alert, rule in alerts_with_rules:
            group_key, group_labels = self._generate_group_key(alert, rule)
            redis_key = self._get_group_key(group_key, is_recovery=False)
            
            if redis_key not in groups_map:
                groups_map[redis_key] = {
                    "group_key": group_key,
                    "group_labels": group_labels,
                    "rule_id": rule.id,
                    "rule_name": rule.name,
                    "alerts": [],
                    "redis_key": redis_key
                }
                redis_keys_to_fetch.add(redis_key)
        
        # æ‰¹é‡ä» Redis è¯»å–ç°æœ‰åˆ†ç»„ï¼ˆä½¿ç”¨ Pipelineï¼‰
        if redis_keys_to_fetch:
            async with self.redis.pipeline(transaction=False) as pipe:
                for redis_key in redis_keys_to_fetch:
                    # å…ˆæ£€æŸ¥ç¼“å­˜
                    if not self._is_cache_valid(redis_key):
                        pipe.get(redis_key)
                
                results = await pipe.execute()
                
                # å¤„ç†ç»“æœ
                result_idx = 0
                for redis_key in redis_keys_to_fetch:
                    if self._is_cache_valid(redis_key):
                        # ä½¿ç”¨ç¼“å­˜æ•°æ®
                        cached_group = self.group_cache.get(redis_key)
                        if cached_group:
                            groups_map[redis_key].update(cached_group)
                    elif result_idx < len(results) and results[result_idx]:
                        # ä½¿ç”¨ Redis æ•°æ®
                        existing_group = json.loads(results[result_idx])
                        groups_map[redis_key].update(existing_group)
                        # æ›´æ–°ç¼“å­˜
                        self.group_cache[redis_key] = existing_group
                        self.cache_timestamps[redis_key] = time.time()
                        result_idx += 1
        
        # æ·»åŠ å‘Šè­¦åˆ°å¯¹åº”åˆ†ç»„
        current_time = time.time()
        for alert, rule in alerts_with_rules:
            group_key, _ = self._generate_group_key(alert, rule)
            redis_key = self._get_group_key(group_key, is_recovery=False)
            group = groups_map[redis_key]
            
            # åˆå§‹åŒ–æ—¶é—´æˆ³ï¼ˆå¦‚æœæ˜¯æ–°åˆ†ç»„ï¼‰
            if "created_at" not in group:
                group["created_at"] = current_time
                group["last_updated_at"] = current_time
                group["sent"] = False
            
            # æ·»åŠ å‘Šè­¦ï¼ˆé¿å…é‡å¤ï¼‰
            alert_fingerprints = {a["fingerprint"] for a in group["alerts"]}
            if alert.fingerprint not in alert_fingerprints:
                group["alerts"].append({
                    "fingerprint": alert.fingerprint,
                    "rule_name": alert.rule_name,
                    "severity": alert.severity,
                    "value": alert.value,
                    "labels": alert.labels,
                    "annotations": alert.annotations,
                    "started_at": alert.started_at,
                    "expr": alert.expr,
                    "tenant_id": alert.tenant_id
                })
                group["last_updated_at"] = current_time
        
        # æ‰¹é‡å†™å…¥ Redisï¼ˆä½¿ç”¨ Pipelineï¼‰
        async with self.redis.pipeline(transaction=True) as pipe:
            for redis_key, group in groups_map.items():
                # ç§»é™¤ä¸´æ—¶å­—æ®µ
                group_to_save = {k: v for k, v in group.items() if k != "redis_key"}
                pipe.setex(redis_key, 7200, json.dumps(group_to_save))
                
                # æ›´æ–°ç¼“å­˜
                self.group_cache[redis_key] = group_to_save
                self.cache_timestamps[redis_key] = current_time
            
            await pipe.execute()
        
        elapsed = time.time() - start_time
        logger.info(f"âœ… æ‰¹é‡æ·»åŠ å®Œæˆ: {len(alerts_with_rules)} ä¸ªå‘Šè­¦, {len(groups_map)} ä¸ªåˆ†ç»„, è€—æ—¶ {elapsed:.3f}s")
        
        return list(groups_map.keys())
    
    async def add_recovery_alert(self, alert: AlertEvent, rule: AlertRule) -> str:
        """
        æ·»åŠ æ¢å¤å‘Šè­¦åˆ°åˆ†ç»„ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        è¿”å›: group_key
        """
        async with self.semaphore:  # å¹¶å‘æ§åˆ¶
            group_key, group_labels = self._generate_group_key(alert, rule)
            recovery_key = f"recovery:{group_key}"
            redis_key = self._get_group_key(recovery_key, is_recovery=True)
            
            # ä»ç¼“å­˜æˆ– Redis è·å–ç°æœ‰åˆ†ç»„æ•°æ®
            group = await self._get_group_from_cache_or_redis(redis_key)
            current_time = time.time()
            
            if not group:
                # åˆ›å»ºæ–°æ¢å¤åˆ†ç»„
                group = {
                    "group_key": recovery_key,
                    "group_labels": group_labels,
                    "rule_id": rule.id,
                    "rule_name": rule.name,
                    "alerts": [],
                    "created_at": current_time,
                    "last_updated_at": current_time,
                    "sent": False
                }
                logger.debug(f"åˆ›å»ºæ–°çš„æ¢å¤å‘Šè­¦åˆ†ç»„: {recovery_key}")
            
            # æ·»åŠ æ¢å¤å‘Šè­¦
            alert_fingerprints = {a["fingerprint"] for a in group["alerts"]}
            if alert.fingerprint not in alert_fingerprints:
                group["alerts"].append({
                    "fingerprint": alert.fingerprint,
                    "rule_name": alert.rule_name,
                    "severity": alert.severity,
                    "value": alert.value,
                    "labels": alert.labels,
                    "annotations": alert.annotations,
                    "started_at": alert.started_at,
                    "expr": alert.expr,
                    "tenant_id": alert.tenant_id
                })
                group["last_updated_at"] = current_time
                
                # ä½¿ç”¨ Pipeline å†™å…¥
                async with self.redis.pipeline(transaction=True) as pipe:
                    pipe.setex(redis_key, 7200, json.dumps(group))
                    await pipe.execute()
                
                # æ›´æ–°ç¼“å­˜
                self.group_cache[redis_key] = group
                self.cache_timestamps[redis_key] = current_time
                
                logger.debug(f"æ¢å¤å‘Šè­¦æ·»åŠ åˆ°åˆ†ç»„: {recovery_key}, å½“å‰å‘Šè­¦æ•°: {len(group['alerts'])}")
            
            return recovery_key
    
    async def get_ready_groups(self) -> List[tuple]:
        """
        è·å–å‡†å¤‡å¥½å‘é€çš„åˆ†ç»„ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        è¿”å›: List[tuple(group_data, is_recovery)]
        """
        ready_groups = []
        current_time = time.time()
        
        # ä½¿ç”¨ Pipeline æ‰¹é‡è¯»å–
        firing_keys = []
        recovery_keys = []
        
        # æ”¶é›†æ‰€æœ‰é”®
        async for key in self.redis.scan_iter(match=f"{self.firing_prefix}:*", count=100):
            firing_keys.append(key)
        
        async for key in self.redis.scan_iter(match=f"{self.recovery_prefix}:*", count=100):
            recovery_keys.append(key)
        
        # æ‰¹é‡è¯»å– firing åˆ†ç»„
        if firing_keys:
            async with self.redis.pipeline(transaction=False) as pipe:
                for key in firing_keys:
                    pipe.get(key)
                results = await pipe.execute()
                
                for group_data in results:
                    if group_data:
                        group = json.loads(group_data)
                        if self._is_group_ready(group, current_time):
                            ready_groups.append((group, False))
                            logger.debug(f"âœ… firing åˆ†ç»„å‡†å¤‡å°±ç»ª: {group['group_key']}, å‘Šè­¦æ•°: {len(group['alerts'])}")
        
        # æ‰¹é‡è¯»å– recovery åˆ†ç»„
        if recovery_keys:
            async with self.redis.pipeline(transaction=False) as pipe:
                for key in recovery_keys:
                    pipe.get(key)
                results = await pipe.execute()
                
                for group_data in results:
                    if group_data:
                        group = json.loads(group_data)
                        if self._is_group_ready(group, current_time):
                            ready_groups.append((group, True))
                            logger.debug(f"âœ… recovery åˆ†ç»„å‡†å¤‡å°±ç»ª: {group['group_key']}, å‘Šè­¦æ•°: {len(group['alerts'])}")
        
        return ready_groups
    
    def _is_group_ready(self, group: dict, current_time: float) -> bool:
        """æ£€æŸ¥åˆ†ç»„æ˜¯å¦å‡†å¤‡å¥½å‘é€"""
        if not group.get("alerts"):
            return False
        
        if group.get("sent"):
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å¤å‘é€
            if (current_time - group["last_updated_at"]) >= self.repeat_interval:
                return True
            return False
        
        # æ£€æŸ¥ç­‰å¾…æ—¶é—´
        wait_time = current_time - group["created_at"]
        return wait_time >= self.group_wait
    
    async def mark_group_sent(self, group_key: str, is_recovery: bool = False):
        """æ ‡è®°åˆ†ç»„ä¸ºå·²å‘é€ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        redis_key = self._get_group_key(group_key, is_recovery)
        
        # ä»ç¼“å­˜æˆ– Redis è·å–
        group = await self._get_group_from_cache_or_redis(redis_key)
        
        if group:
            group["sent"] = True
            
            # ä½¿ç”¨ Pipeline å†™å…¥
            async with self.redis.pipeline(transaction=True) as pipe:
                pipe.setex(redis_key, 7200, json.dumps(group))
                await pipe.execute()
            
            # æ›´æ–°ç¼“å­˜
            self.group_cache[redis_key] = group
            self.cache_timestamps[redis_key] = time.time()
    
    async def clear_sent_group(self, group_key: str, is_recovery: bool = False):
        """æ¸…é™¤å·²å‘é€çš„åˆ†ç»„"""
        redis_key = self._get_group_key(group_key, is_recovery)
        
        # ä½¿ç”¨ Pipeline åˆ é™¤
        async with self.redis.pipeline(transaction=True) as pipe:
            pipe.delete(redis_key)
            await pipe.execute()
        
        # æ¸…é™¤ç¼“å­˜
        self._invalidate_cache(redis_key)
        logger.debug(f"æ¸…é™¤å·²å‘é€åˆ†ç»„: {group_key}")
    
    async def remove_alert_from_groups(self, fingerprint: str):
        """ä»æ‰€æœ‰åˆ†ç»„ä¸­ç§»é™¤æŒ‡å®šçš„å‘Šè­¦ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        # æ”¶é›†æ‰€æœ‰éœ€è¦æ›´æ–°çš„é”®
        keys_to_update = []
        
        async for key in self.redis.scan_iter(match=f"{self.firing_prefix}:*", count=100):
            keys_to_update.append(key)
        
        if not keys_to_update:
            return
        
        # æ‰¹é‡è¯»å–
        async with self.redis.pipeline(transaction=False) as pipe:
            for key in keys_to_update:
                pipe.get(key)
            results = await pipe.execute()
        
        # å¤„ç†å¹¶æ‰¹é‡å†™å…¥
        async with self.redis.pipeline(transaction=True) as pipe:
            for key, group_data in zip(keys_to_update, results):
                if group_data:
                    group = json.loads(group_data)
                    original_count = len(group["alerts"])
                    group["alerts"] = [a for a in group["alerts"] if a["fingerprint"] != fingerprint]
                    
                    if len(group["alerts"]) < original_count:
                        if group["alerts"]:
                            pipe.setex(key, 7200, json.dumps(group))
                            # æ›´æ–°ç¼“å­˜
                            self.group_cache[key] = group
                            self.cache_timestamps[key] = time.time()
                        else:
                            pipe.delete(key)
                            # æ¸…é™¤ç¼“å­˜
                            self._invalidate_cache(key)
            
            await pipe.execute()
    
    async def get_group_stats(self) -> Dict[str, int]:
        """è·å–åˆ†ç»„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        firing_keys = []
        recovery_keys = []
        
        # æ”¶é›†æ‰€æœ‰é”®
        async for key in self.redis.scan_iter(match=f"{self.firing_prefix}:*", count=100):
            firing_keys.append(key)
        
        async for key in self.redis.scan_iter(match=f"{self.recovery_prefix}:*", count=100):
            recovery_keys.append(key)
        
        total_alerts = 0
        sent_count = 0
        pending_count = 0
        
        # æ‰¹é‡è¯»å– firing åˆ†ç»„
        if firing_keys:
            async with self.redis.pipeline(transaction=False) as pipe:
                for key in firing_keys:
                    pipe.get(key)
                results = await pipe.execute()
                
                for group_data in results:
                    if group_data:
                        group = json.loads(group_data)
                        total_alerts += len(group["alerts"])
                        if group.get("sent"):
                            sent_count += 1
                        else:
                            pending_count += 1
        
        # æ‰¹é‡è¯»å– recovery åˆ†ç»„
        if recovery_keys:
            async with self.redis.pipeline(transaction=False) as pipe:
                for key in recovery_keys:
                    pipe.get(key)
                results = await pipe.execute()
                
                for group_data in results:
                    if group_data:
                        group = json.loads(group_data)
                        total_alerts += len(group["alerts"])
                        if group.get("sent"):
                            sent_count += 1
                        else:
                            pending_count += 1
        
        return {
            "total_groups": len(firing_keys) + len(recovery_keys),
            "firing_groups": len(firing_keys),
            "recovery_groups": len(recovery_keys),
            "total_alerts": total_alerts,
            "sent_groups": sent_count,
            "pending_groups": pending_count,
            "cache_size": len(self.group_cache)
        }
    
    def configure(
        self, 
        group_wait: int = 10, 
        group_interval: int = 30, 
        repeat_interval: int = 3600
    ):
        """é…ç½®åˆ†ç»„å‚æ•°"""
        self.group_wait = group_wait
        self.group_interval = group_interval
        self.repeat_interval = repeat_interval
        logger.info(
            f"ä¼˜åŒ–å‘Šè­¦åˆ†ç»„å™¨é…ç½®: group_wait={group_wait}s, "
            f"group_interval={group_interval}s, repeat_interval={repeat_interval}s"
        )
    
    async def clear_cache(self):
        """æ¸…é™¤å†…å­˜ç¼“å­˜"""
        self.group_cache.clear()
        self.cache_timestamps.clear()
        logger.info("å†…å­˜ç¼“å­˜å·²æ¸…é™¤")